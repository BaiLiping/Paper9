@article{Dao2020AdaptiveRL,
  title={Adaptive Reinforcement Learning Strategy with Sliding Mode Control for Unknown and Disturbed Wheeled Inverted Pendulum},
  author={P. Dao and Yen-Chen Liu},
  journal={International Journal of Control Automation and Systems},
  year={2020}
}

@article{Zheng2020BalanceCF,
  title={Balance Control for the First-order Inverted Pendulum Based on the Advantage Actor-critic Algorithm},
  author={Yan Zheng and Xutong Li and L. Xu},
  journal={International Journal of Control Automation and Systems},
  year={2020},
  volume={18},
  pages={1-8}
}


@article{Choi2017InverseRL,
  title={Inverse reinforcement learning control for trajectory tracking of a multirotor UAV},
  author={S. Choi and S. Kim and H. Jin Kim},
  journal={International Journal of Control, Automation and Systems},
  year={2017},
  volume={15},
  pages={1826-1834}
}

@article{Lv2019ApproximateOS,
  title={Approximate Optimal Stabilization Control of Servo Mechanisms based on Reinforcement Learning Scheme},
  author={Yongfeng Lv and X. Ren and Shuangyi Hu and H. Xu},
  journal={International Journal of Control Automation and Systems},
  year={2019},
  volume={17},
  pages={2655-2665}
}



@article{Cheng2019OnorbitRU,
  title={On-orbit Reconfiguration Using Adaptive Dynamic Programming for Multi-mission-constrained Spacecraft Attitude Control System},
  author={Yuehua Cheng and B. Jiang and H. Li and Xiao-Dong Han},
  journal={International Journal of Control, Automation and Systems},
  year={2019},
  volume={17},
  pages={822-835}
}
@article{Oh2020DeepRB,
  title={Deep RL Based Notch Filter Design Method for Complex Industrial Servo Systems},
  author={Tae-Ho Oh and Ji-Seok Han and Y. Kim and Dae-Young Yang and S. Lee and D. D. Cho},
  journal={International Journal of Control Automation and Systems},
  year={2020},
  volume={18},
  pages={1-10}
}

@article{Dornheim2018ModelfreeAO,
  title={Model-free Adaptive Optimal Control of Episodic Fixed-horizon Manufacturing Processes Using Reinforcement Learning},
  author={Johannes Dornheim and N. Link and P. Gumbsch},
  journal={International Journal of Control, Automation and Systems},
  year={2018},
  volume={18},
  pages={1593-1604}
}

@article{Xin2020RobustES,
  title={Robust Experimental Study of Data-driven Optimal Control for an Underactuated Rotary Flexible Joint},
  author={Ying Xin and Zhi-Chang Qin and Jian-Qiao Sun},
  journal={International Journal of Control, Automation and Systems},
  year={2020},
  volume={18},
  pages={1202-1214}
}

@INPROCEEDINGS{6386109,

  author={E. {Todorov} and T. {Erez} and Y. {Tassa}},

  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 

  title={MuJoCo: A physics engine for model-based control}, 

  year={2012},

  volume={},

  number={},

  pages={5026-5033},

  doi={10.1109/IROS.2012.6386109}}

@article{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}



@article{Xie2018LearningWT,
  title={Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning},
  author={Linhai Xie and S. Wang and S. Rosa and A. Markham and A. Trigoni},
  journal={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2018},
  pages={6276-6283}
}
@article{Fachantidis2019LearningTT,
  title={Learning to Teach Reinforcement Learning Agents},
  author={A. Fachantidis and Matthew E. Taylor and I. Vlahavas},
  journal={ArXiv},
  year={2019},
  volume={abs/1707.09079}
}


@inproceedings{Knox2010CombiningMF,
author = {Knox, W. Bradley and Stone, Peter},
title = {Combining Manual Feedback with Subsequent MDP Reward Signals for Reinforcement Learning},
year = {2010},
isbn = {9780982657119},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As learning agents move from research labs to the real world, it is increasingly important that human users, including those without programming skills, be able to teach agents desired behaviors. Recently, the tamer framework was introduced for designing agents that can be interactively shaped by human trainers who give only positive and negative feedback signals. Past work on tamer showed that shaping can greatly reduce the sample complexity required to learn a good policy, can enable lay users to teach agents the behaviors they desire, and can allow agents to learn within a Markov Decision Process (MDP) in the absence of a coded reward function. However, tamer does not allow this human training to be combined with autonomous learning based on such a coded reward function. This paper leverages the fast learning exhibited within the tamer framework to hasten a reinforcement learning (RL) algorithm's climb up the learning curve, effectively demonstrating that human reinforcement and MDP reward can be used in conjunction with one another by an autonomous agent. We tested eight plausible tamer+rl methods for combining a previously learned human reinforcement function, H, with MDP reward in a reinforcement learning algorithm. This paper identifies which of these methods are most effective and analyzes their strengths and weaknesses. Results from these tamer+rl algorithms indicate better final performance and better cumulative performance than either a tamer agent or an RL agent alone.},
booktitle = {Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1},
pages = {5–12},
numpages = {8},
keywords = {human teachers, human-agent interaction, shaping, reinforcement learning},
location = {Toronto, Canada},
series = {AAMAS '10}
}


@inproceedings{Knox2009InteractivelySA,
author = {Knox, W. Bradley and Stone, Peter},
title = {Interactively Shaping Agents via Human Reinforcement: The TAMER Framework},
year = {2009},
isbn = {9781605586588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1597735.1597738},
doi = {10.1145/1597735.1597738},
abstract = {As computational learning agents move into domains that incur real costs (e.g., autonomous driving or financial investment), it will be necessary to learn good policies without numerous high-cost learning trials. One promising approach to reducing sample complexity of learning a task is knowledge transfer from humans to agents. Ideally, methods of transfer should be accessible to anyone with task knowledge, regardless of that person's expertise in programming and AI. This paper focuses on allowing a human trainer to interactively shape an agent's policy via reinforcement signals. Specifically, the paper introduces "Training an Agent Manually via Evaluative Reinforcement," or TAMER, a framework that enables such shaping. Differing from previous approaches to interactive shaping, a TAMER agent models the human's reinforcement and exploits its model by choosing actions expected to be most highly reinforced. Results from two domains demonstrate that lay users can train TAMER agents without defining an environmental reward function (as in an MDP) and indicate that human training within the TAMER framework can reduce sample complexity over autonomous learning algorithms.},
booktitle = {Proceedings of the Fifth International Conference on Knowledge Capture},
pages = {9–16},
numpages = {8},
keywords = {sequential decision-making, human-agent interaction, learning agents, human teachers, shaping},
location = {Redondo Beach, California, USA},
series = {K-CAP '09}
}





@article{Carlucho2017IncrementalQS,
  title={Incremental Q-learning strategy for adaptive PID control of mobile robots},
  author={Ignacio Carlucho and M. D. Paula and Sebasti{\'a}n A. Villar and G. G. Acosta},
  journal={Expert Syst. Appl.},
  year={2017},
  volume={80},
  pages={183-199}
}
@article{Han2020ActorCriticRL,
  title={Actor-Critic Reinforcement Learning for Control With Stability Guarantee},
  author={Minghao Han and L. Zhang and J. Wang and Weike Pan},
  journal={IEEE Robotics and Automation Letters},
  year={2020},
  volume={5},
  pages={6217-6224}
}
@article{Pavse2020RIDMRI,
  title={RIDM: Reinforced Inverse Dynamics Modeling for Learning from a Single Observed Demonstration},
  author={Brahma S. Pavse and F. Torabi and Josiah P. Hanna and Garrett Warnell and P. Stone},
  journal={IEEE Robotics and Automation Letters},
  year={2020},
  volume={5},
  pages={6262-6269}
}



@article{Recht2018ATO,
  title={A Tour of Reinforcement Learning: The View from Continuous Control},
  author={B. Recht},
  journal={ArXiv},
  year={2018},
  volume={abs/1806.09460}
}
@article{Osband2016DeepEV,
  title={Deep Exploration via Bootstrapped DQN},
  author={Ian Osband and Charles Blundell and A. Pritzel and Benjamin Van Roy},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.04621}
}

@article{Kaufmann2020DeepDA,
  title={Deep Drone Acrobatics},
  author={E. Kaufmann and Antonio Loquercio and Ren'e Ranftl and M. M{\"u}ller and V. Koltun and D. Scaramuzza},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.05768}
}
@article{Mnih2013PlayingAW,
  title={Playing Atari with Deep Reinforcement Learning},
  author={V. Mnih and K. Kavukcuoglu and D. Silver and A. Graves and Ioannis Antonoglou and Daan Wierstra and Martin A. Riedmiller},
  journal={ArXiv},
  year={2013},
  volume={abs/1312.5602}
}
@article{Nachum2020ReinforcementLV,
  title={Reinforcement Learning via Fenchel-Rockafellar Duality},
  author={Ofir Nachum and Bo Dai},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.01866}
}
@article{BelbutePeres2020CombiningDP,
  title={Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction},
  author={Filipe de Avila Belbute-Peres and Thomas D. Economon and J. Z. Kolter},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.04439}
}
@article{Bai2019DeepEM,
  title={Deep Equilibrium Models},
  author={Shaojie Bai and J. Z. Kolter and V. Koltun},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.01377}
}

@article{Wu2020DataDrivenDL,
  title={Data-Driven Deep Learning of Partial Differential Equations in Modal Space},
  author={K. Wu and D. Xiu},
  journal={ArXiv},
  year={2020},
  volume={abs/1910.06948}
}
@article{Li2020FourierNO,
  title={Fourier Neural Operator for Parametric Partial Differential Equations},
  author={Zong-Yi Li and Nikola B. Kovachki and Kamyar Azizzadenesheli and B. Liu and K. Bhattacharya and A. Stuart and Anima Anandkumar},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.08895}
}
@inproceedings{Luo2019ADR,
  title={A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer},
  author={Fuli Luo and Peng Li and J. Zhou and Pengcheng Yang and Baobao Chang and Zhifang Sui and X. Sun},
  booktitle={IJCAI},
  year={2019}
}
@article{Andrychowicz2020LearningDI,
  title={Learning dexterous in-hand manipulation},
  author={OpenAI Marcin Andrychowicz and Bowen Baker and Maciek Chociej and R. J{\'o}zefowicz and Bob McGrew and Jakub W. Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and J. Schneider and S. Sidor and Josh Tobin and P. Welinder and Lilian Weng and W. Zaremba},
  journal={The International Journal of Robotics Research},
  year={2020},
  volume={39},
  pages={20 - 3}
}
@inproceedings{Weinan2017APO,
  title={A Proposal on Machine Learning via Dynamical Systems},
  author={E. Weinan},
  year={2017}
}
@inproceedings{Dupont2019AugmentedNO,
  title={Augmented Neural ODEs},
  author={E. Dupont and A. Doucet and Y. Teh},
  booktitle={NeurIPS},
  year={2019}
}
@article{Betancourt2018OnSO,
  title={On Symplectic Optimization},
  author={M. Betancourt and Michael I. Jordan and A. Wilson},
  journal={arXiv: Computation},
  year={2018}
}
@article{Kalashnikov2018QTOptSD,
  title={QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},
  author={D. Kalashnikov and A. Irpan and P. Pastor and J. Ibarz and A. Herzog and Eric Jang and Deirdre Quillen and Ethan Holly and Mrinal Kalakrishnan and V. Vanhoucke and S. Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1806.10293}
}
@article{Burda2019LargeScaleSO,
  title={Large-Scale Study of Curiosity-Driven Learning},
  author={Yuri Burda and H. Edwards and Deepak Pathak and A. Storkey and Trevor Darrell and Alexei A. Efros},
  journal={ArXiv},
  year={2019},
  volume={abs/1808.04355}
}
@article{Lee2020LearningQL,
  title={Learning quadrupedal locomotion over challenging terrain},
  author={Joonho Lee and J. Hwangbo and Lorenz Wellhausen and V. Koltun and M. Hutter},
  journal={Science Robotics},
  year={2020},
  volume={5}
}

@article{Szegedy2017Inceptionv4IA,
  title={Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
  author={Christian Szegedy and S. Ioffe and V. Vanhoucke and Alexander Amir Alemi},
  journal={ArXiv},
  year={2017},
  volume={abs/1602.07261}
}




@article{doi:10.1146/annurev-control-090419-075625,
author = {Hewing, Lukas and Wabersich, Kim P. and Menner, Marcel and Zeilinger, Melanie N.},
title = {Learning-Based Model Predictive Control: Toward Safe Learning in Control},
journal = {Annual Review of Control, Robotics, and Autonomous Systems},
volume = {3},
number = {1},
pages = {269-296},
year = {2020},
doi = {10.1146/annurev-control-090419-075625},

URL = { 
        https://doi.org/10.1146/annurev-control-090419-075625
    
},
eprint = { 
        https://doi.org/10.1146/annurev-control-090419-075625
    
}
,
    abstract = { Recent successes in the field of machine learning, as well as the availability of increased sensing and computational capabilities in modern control systems, have led to a growing interest in learning and data-driven control techniques. Model predictive control (MPC), as the prime methodology for constrained control, offers a significant opportunity to exploit the abundance of data in a reliable manner, particularly while taking safety constraints into account. This review aims at summarizing and categorizing previous research on learning-based MPC, i.e., the integration or combination of MPC with learning methods, for which we consider three main categories. Most of the research addresses learning for automatic improvement of the prediction model from recorded data. There is, however, also an increasing interest in techniques to infer the parameterization of the MPC controller, i.e., the cost and constraints, that lead to the best closed-loop performance. Finally, we discuss concepts that leverage MPC to augment learning-based controllers with constraint satisfaction properties. }
}



@inproceedings{Knox2010CombiningMF,
  title={Combining manual feedback with subsequent MDP reward signals for reinforcement learning},
  author={W. B. Knox and P. Stone},
  booktitle={AAMAS},
  year={2010}
}

@article{Lusch2018DeepLF,
  title={Deep learning for universal linear embeddings of nonlinear dynamics},
  author={Bethany Lusch and J. N. Kutz and S. Brunton},
  journal={Nature Communications},
  year={2018},
  volume={9}
}
@inproceedings{Zhang2020DesigningOD,
  title={Designing Optimal Dynamic Treatment Regimes: A Causal Reinforcement Learning Approach},
  author={J. Zhang},
  booktitle={ICML 2020},
  year={2020}
}
@misc{tensorforce,
  author       = {Kuhnle, Alexander and Schaarschmidt, Michael and Fricke, Kai},
  title        = {Tensorforce: a TensorFlow library for applied reinforcement learning},
  howpublished = {Web page},
  url          = {https://github.com/tensorforce/tensorforce},
  year         = {2017}
}
@article{Brockman2016OpenAIG,
  title={OpenAI Gym},
  author={G. Brockman and Vicki Cheung and Ludwig Pettersson and J. Schneider and John Schulman and Jie Tang and W. Zaremba},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.01540}
}
@article{Nickolls2008ScalablePP,
  title={Scalable parallel programming with CUDA},
  author={John Nickolls and I. Buck and M. Garland and K. Skadron},
  journal={2008 IEEE Hot Chips 20 Symposium (HCS)},
  year={2008},
  pages={1-2}
}
@inproceedings{Paszke2017AutomaticDI,
  title={Automatic differentiation in PyTorch},
  author={Adam Paszke and S. Gross and Soumith Chintala and G. Chanan and E. Yang and Zachary Devito and Zeming Lin and Alban Desmaison and L. Antiga and A. Lerer},
  year={2017}
}
@article{Abadi2016TensorFlowAS,
  title={TensorFlow: A system for large-scale machine learning},
  author={M. Abadi and P. Barham and J. Chen and Z. Chen and Andy Davis and J. Dean and M. Devin and Sanjay Ghemawat and Geoffrey Irving and M. Isard and M. Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and D. Murray and B. Steiner and P. Tucker and V. Vasudevan and Pete Warden and Martin Wicke and Y. Yu and Xiaoqiang Zhang},
  journal={ArXiv},
  year={2016},
  volume={abs/1605.08695}
}
@article{Dasgupta2019CausalRF,
  title={Causal Reasoning from Meta-reinforcement Learning},
  author={Ishita Dasgupta and J. X. Wang and S. Chiappa and Jovana Mitrovic and Pedro A. Ortega and D. Raposo and Edward Hughes and P. Battaglia and M. Botvinick and Z. Kurth-Nelson},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.08162}
}
@article{Vezhnevets2017FeUdalNF,
  title={FeUdal Networks for Hierarchical Reinforcement Learning},
  author={A. S. Vezhnevets and Simon Osindero and T. Schaul and N. Heess and Max Jaderberg and D. Silver and K. Kavukcuoglu},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.01161}
}
@article{Nachum2018DataEfficientHR,
  title={Data-Efficient Hierarchical Reinforcement Learning},
  author={Ofir Nachum and Shixiang Gu and H. Lee and S. Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1805.08296}
}
@article{Pathak2017CuriosityDrivenEB,
  title={Curiosity-Driven Exploration by Self-Supervised Prediction},
  author={Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2017},
  pages={488-489}
}
@article{Peng2018DeepMimicED,
  title={DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills},
  author={X. Peng and P. Abbeel and S. Levine and M. V. D. Panne},
  journal={ACM Trans. Graph.},
  year={2018},
  volume={37},
  pages={143:1-143:14}
}
@article{Peng2020LearningAR,
  title={Learning Agile Robotic Locomotion Skills by Imitating Animals},
  author={X. Peng and Erwin Coumans and T. Zhang and T. Lee and J. Tan and Sergey Levine},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.00784}
}

@article{Paine2018OneShotHI,
  title={One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL},
  author={T. Paine and Sergio Gomez Colmenarejo and Ziyu Wang and S. Reed and Y. Aytar and T. Pfaff and M. W. Hoffman and Gabriel Barth-Maron and Serkan Cabi and D. Budden and N. D. Freitas},
  journal={ArXiv},
  year={2018},
  volume={abs/1810.05017}
}

@inproceedings{Mishra2018ASN,
  title={A Simple Neural Attentive Meta-Learner},
  author={N. Mishra and Mostafa Rohaninejad and Xi Chen and P. Abbeel},
  booktitle={ICLR},
  year={2018}
}



@inproceedings{Hausknecht2015DeepRQ,
  title={Deep Recurrent Q-Learning for Partially Observable MDPs},
  author={Matthew J. Hausknecht and P. Stone},
  booktitle={AAAI Fall Symposia},
  year={2015}
}





@article{Powell2009WhatYS,
  title={What You Should Know About Approximate Dynamic Programming},
  author={W. Powell},
  journal={Naval Research Logistics},
  year={2009},
  volume={56},
  pages={239-249}
}
@inproceedings{Bertsekas2018ReinforcementLA,
  title={Reinforcement Learning and Optimal Control by},
  author={D. P. Bertsekas},
  year={2018}
}

 @InProceedings{pmlr-v120-ahmadi20a, title = {Learning Dynamical Systems with Side Information}, author = {Ahmadi, Amir Ali and Khadir, Bachir El}, pages = {718--727}, year = {2020}, editor = {Alexandre M. Bayen and Ali Jadbabaie and George Pappas and Pablo A. Parrilo and Benjamin Recht and Claire Tomlin and Melanie Zeilinger}, volume = {120}, series = {Proceedings of Machine Learning Research}, address = {The Cloud}, month = {10--11 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v120/ahmadi20a/ahmadi20a.pdf}, url = {http://proceedings.mlr.press/v120/ahmadi20a.html}, abstract = { We present a mathematical formalism and a computational framework for the problem of learning a dynamical system from noisy observations of a few trajectories and subject to side information (e.g., physical laws or contextual knowledge). We identify six classes of side information which can be imposed by semidefinite programming and that arise naturally in many applications. We demonstrate their value on two examples from epidemiology and physics. Some density results on polynomial dynamical systems that either exactly or approximately satisfy side information are also presented. } }
 @InProceedings{pmlr-v120-allibhoy20a, title = {Data-Driven Distributed Predictive Control via Network Optimization}, author = {Allibhoy, Ahmed and Cortes, Jorge}, pages = {838--839}, year = {2020}, editor = {Alexandre M. Bayen and Ali Jadbabaie and George Pappas and Pablo A. Parrilo and Benjamin Recht and Claire Tomlin and Melanie Zeilinger}, volume = {120}, series = {Proceedings of Machine Learning Research}, address = {The Cloud}, month = {10--11 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v120/allibhoy20a/allibhoy20a.pdf}, url = {http://proceedings.mlr.press/v120/allibhoy20a.html}, abstract = {We consider a networked linear system where system matrices are unknown to the individual agents but sampled data is available to them. We propose a data-driven method for designing a distributed linear-quadratic controller where agents learn a non-parametric system model from a single sample trajectory in which nodes can predict future trajectories using only data available to themselves and their neighbors. Based on this system representation, we propose a control scheme where a network optimization problem is solved in a receding horizon manner. We show that the proposed control scheme is stabilizing and validate our results through numerical experiments.} }

 @article{Finn2016UnsupervisedLF,
   title={Unsupervised Learning for Physical Interaction through Video Prediction},
   author={Chelsea Finn and Ian J. Goodfellow and S. Levine},
   journal={ArXiv},
   year={2016},
   volume={abs/1605.07157}
 }

 @article{Finn2017ModelAgnosticMF,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Chelsea Finn and P. Abbeel and S. Levine},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.03400}
}
@article{Hornik1991ApproximationCO,
  title={Approximation capabilities of multilayer feedforward networks},
  author={K. Hornik},
  journal={Neural Networks},
  year={1991},
  volume={4},
  pages={251-257}
}
@INPROCEEDINGS{8463189,

  author={A. {Nagabandi} and G. {Kahn} and R. S. {Fearing} and S. {Levine}},

  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},

  title={Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning},

  year={2018},

  volume={},

  number={},

  pages={7559-7566},}

  @article{BelbutePeres2020CombiningDP,
    title={Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction},
    author={Filipe de Avila Belbute-Peres and Thomas D. Economon and J. Z. Kolter},
    journal={ArXiv},
    year={2020},
    volume={abs/2007.04439}
  }
  @article{Agrawal2019DifferentiableCO,
    title={Differentiable Convex Optimization Layers},
    author={Akshay Agrawal and Brandon Amos and S. Barratt and Stephen P. Boyd and Steven Diamond and J. Z. Kolter},
    journal={ArXiv},
    year={2019},
    volume={abs/1910.12430}
  }
  @inproceedings{BelbutePeres2018EndtoEndDP,
    title={End-to-End Differentiable Physics for Learning and Control},
    author={Filipe de Avila Belbute-Peres and K. Smith and Kelsey R. Allen and J. Tenenbaum and J. Z. Kolter},
    booktitle={NeurIPS},
    year={2018}
  }
  @article{Shi2019NeuralLS,
    title={Neural Lander: Stable Drone Landing Control Using Learned Dynamics},
    author={G. Shi and Xichen Shi and M. O'Connell and R. Yu and Kamyar Azizzadenesheli and Anima Anandkumar and Yisong Yue and Soon-Jo Chung},
    journal={2019 International Conference on Robotics and Automation (ICRA)},
    year={2019},
    pages={9784-9790}
  }
  @inproceedings{Sutton1998IntroductionTR,
  title={Introduction to Reinforcement Learning},
  author={R. Sutton and A. Barto},
  year={1998}
}
@article{Kullback1951ONIA,
  title={ON INFORMATION AND SUFFICIENCY},
  author={S. Kullback and R. A. Leibler},
  journal={Annals of Mathematical Statistics},
  year={1951},
  volume={22},
  pages={79-86}
}
@inproceedings{Sutton1998IntroductionTR,
  title={Introduction to Reinforcement Learning},
  author={R. Sutton and A. Barto},
  year={1998}
}
@article{Levine2018ReinforcementLA,
  title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review},
  author={S. Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1805.00909}
}
@inproceedings{Ho2016GenerativeAI,
  title={Generative Adversarial Imitation Learning},
  author={Jonathan Ho and S. Ermon},
  booktitle={NIPS},
  year={2016}
}
@inproceedings{Hewing2020LearningBasedMP,
  title={Learning-Based Model Predictive Control: Toward Safe Learning in Control},
  author={Lukas Hewing and K. P. Wabersich and M. Menner and Melanie N. Zeilinger},
  year={2020}
}

@article{Mohan2020EmbeddingHP,
  title={Embedding Hard Physical Constraints in Neural Network Coarse-Graining of 3D Turbulence.},
  author={A. Mohan and N. Lubbers and D. Livescu and M. Chertkov},
  journal={arXiv: Computational Physics},
  year={2020}
}

@article{Blundell2015WeightUI,
  title={Weight Uncertainty in Neural Networks},
  author={Charles Blundell and Julien Cornebise and K. Kavukcuoglu and Daan Wierstra},
  journal={ArXiv},
  year={2015},
  volume={abs/1505.05424}
}

@inproceedings{Schulman2015TrustRP,
  title={Trust Region Policy Optimization},
  author={John Schulman and S. Levine and P. Abbeel and Michael I. Jordan and P. Moritz},
  booktitle={ICML},
  year={2015}
}
@inproceedings{Gal2017ConcreteD,
  title={Concrete Dropout},
  author={Yarin Gal and J. Hron and Alex Kendall},
  booktitle={NIPS},
  year={2017}

}
@article{Williams1992SimpleSG,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={R. J. Williams},
  journal={Machine Learning},
  year={1992},
  volume={8},
  pages={229-256}
}




@inproceedings{Bertsekas1996NeuroDynamicP,
  title={Neuro-Dynamic Programming},
  author={D. Bertsekas and J. Tsitsiklis},
  booktitle={Encyclopedia of Machine Learning},
  year={1996}
}
